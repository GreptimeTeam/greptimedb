use std::any::Any;
use std::fmt::Debug;
use std::sync::Arc;

use async_trait::async_trait;
use common_recordbatch::adapter::{DfRecordBatchStreamAdapter, RecordBatchStreamAdapter};
use common_recordbatch::DfSendableRecordBatchStream;
use common_recordbatch::SendableRecordBatchStream;
use datafusion::arrow::datatypes::SchemaRef as DfSchemaRef;
use datafusion::error::Result as DfResult;
use datafusion::execution::runtime_env::RuntimeEnv;
use datafusion::physical_plan::{Partitioning, Statistics};
use datatypes::schema::SchemaRef;
use snafu::ResultExt;

use crate::error::{self, Result};
use crate::DfExecutionPlan;
use crate::PhysicalSortExpr;

pub type ExecutionPlanRef = Arc<dyn ExecutionPlan>;

/// `ExecutionPlan` represent nodes in the Physical Plan.
///
/// Each `ExecutionPlan` is Partition-aware and is responsible for
/// creating the actual `async` [`SendableRecordBatchStream`]s
/// of [`RecordBatch`] that incrementally compute the operator's
/// output from its input partition.
///
/// [`ExecutionPlan`] can be displayed in an simplified form using the
/// return value from [`displayable`] in addition to the (normally
/// quite verbose) `Debug` output.
#[async_trait]
pub trait ExecutionPlan: Debug + Send + Sync {
    /// Returns the execution plan as [`Any`](std::any::Any) so that it can be
    /// downcast to a specific implementation.
    fn as_any(&self) -> &dyn Any;

    /// Get the schema for this execution plan
    fn schema(&self) -> SchemaRef;

    /// Specifies the output partitioning scheme of this plan
    fn output_partitioning(&self) -> Partitioning;

    /// If the output of this operator is sorted, returns `Some(keys)`
    /// with the description of how it was sorted.
    ///
    /// For example, Sort, (obviously) produces sorted output as does
    /// SortPreservingMergeStream. Less obviously `Projection`
    /// produces sorted output if its input was sorted as it does not
    /// reorder the input rows,
    ///
    /// It is safe to return `None` here if your operator does not
    /// have any particular output order here
    fn output_ordering(&self) -> Option<&[PhysicalSortExpr]>;

    /// Get a list of child execution plans that provide the input for this plan. The returned list
    /// will be empty for leaf nodes, will contain a single value for unary nodes, or two
    /// values for binary nodes (such as joins).
    fn children(&self) -> Vec<ExecutionPlanRef>;

    /// Returns a new plan where all children were replaced by new plans.
    /// The size of `children` must be equal to the size of `ExecutionPlan::children()`.
    fn with_new_children(&self, children: Vec<ExecutionPlanRef>) -> Result<ExecutionPlanRef>;

    /// Creates an RecordBatch stream.
    async fn execute(
        &self,
        partition: usize,
        runtime: Arc<RuntimeEnv>,
    ) -> Result<SendableRecordBatchStream>;

    /// Returns the global output statistics for this `ExecutionPlan` node.
    fn statistics(&self) -> Statistics;
}

#[derive(Debug)]
pub struct ExecutionPlanAdapter(pub Arc<dyn DfExecutionPlan>);

#[async_trait]
impl ExecutionPlan for ExecutionPlanAdapter {
    fn as_any(&self) -> &dyn Any {
        self
    }

    fn schema(&self) -> SchemaRef {
        // DataFusion's schema should always able to cast to our schema here, because it's
        // generated by our query engine and used in adaptor.
        Arc::new(self.0.schema().try_into().unwrap())
    }

    fn output_partitioning(&self) -> Partitioning {
        self.0.output_partitioning()
    }

    fn output_ordering(&self) -> Option<&[PhysicalSortExpr]> {
        None
    }

    fn children(&self) -> Vec<ExecutionPlanRef> {
        self.0
            .children()
            .into_iter()
            .map(|x| Arc::new(ExecutionPlanAdapter(x)) as _)
            .collect()
    }

    fn with_new_children(&self, children: Vec<ExecutionPlanRef>) -> Result<ExecutionPlanRef> {
        let children = children
            .into_iter()
            .map(|x| Arc::new(DfExecutionPlanAdapter(x)) as _)
            .collect();
        let plan = self
            .0
            .with_new_children(children)
            .context(error::GeneralDataFusionSnafu)?;
        Ok(Arc::new(ExecutionPlanAdapter(plan)))
    }

    async fn execute(
        &self,
        partition: usize,
        runtime: Arc<RuntimeEnv>,
    ) -> Result<SendableRecordBatchStream> {
        let stream = self
            .0
            .execute(partition, runtime)
            .await
            .context(error::DataFusionExecutionPlanSnafu)?;
        let stream =
            RecordBatchStreamAdapter::try_new(stream).context(error::GeneralRecordBatchSnafu)?;
        Ok(Box::pin(stream))
    }

    fn statistics(&self) -> Statistics {
        self.0.statistics()
    }
}

#[derive(Debug)]
pub struct DfExecutionPlanAdapter(pub ExecutionPlanRef);

#[async_trait]
impl DfExecutionPlan for DfExecutionPlanAdapter {
    fn as_any(&self) -> &dyn Any {
        self
    }

    fn schema(&self) -> DfSchemaRef {
        self.0.schema().arrow_schema().clone()
    }

    fn output_partitioning(&self) -> Partitioning {
        self.0.output_partitioning()
    }

    fn output_ordering(&self) -> Option<&[PhysicalSortExpr]> {
        self.0.output_ordering()
    }

    fn children(&self) -> Vec<Arc<dyn DfExecutionPlan>> {
        self.0
            .children()
            .into_iter()
            .map(|x| Arc::new(DfExecutionPlanAdapter(x)) as _)
            .collect()
    }

    fn with_new_children(
        &self,
        children: Vec<Arc<dyn DfExecutionPlan>>,
    ) -> DfResult<Arc<dyn DfExecutionPlan>> {
        let children = children
            .into_iter()
            .map(|x| Arc::new(ExecutionPlanAdapter(x)) as _)
            .collect();
        let plan = self.0.with_new_children(children)?;
        Ok(Arc::new(DfExecutionPlanAdapter(plan)))
    }

    async fn execute(
        &self,
        partition: usize,
        runtime: Arc<RuntimeEnv>,
    ) -> DfResult<DfSendableRecordBatchStream> {
        let stream = self.0.execute(partition, runtime).await?;
        Ok(Box::pin(DfRecordBatchStreamAdapter::new(stream)))
    }

    fn statistics(&self) -> Statistics {
        self.0.statistics()
    }
}

#[cfg(test)]
mod test {
    use arrow::datatypes::{DataType, Field, Schema as ArrowSchema};
    use common_recordbatch::{RecordBatch, RecordBatches};
    use datafusion::arrow_print;
    use datafusion::datasource::TableProvider as DfTableProvider;
    use datafusion::logical_plan::LogicalPlanBuilder;
    use datafusion::physical_plan::collect;
    use datafusion::prelude::ExecutionContext;
    use datafusion_common::field_util::SchemaExt;
    use datafusion_expr::Expr;
    use datatypes::schema::Schema;
    use datatypes::vectors::Int32Vector;

    use super::*;

    struct MyDfTableProvider;

    #[async_trait]
    impl DfTableProvider for MyDfTableProvider {
        fn as_any(&self) -> &dyn Any {
            self
        }

        fn schema(&self) -> DfSchemaRef {
            Arc::new(ArrowSchema::new(vec![Field::new(
                "a",
                DataType::Int32,
                false,
            )]))
        }

        async fn scan(
            &self,
            _projection: &Option<Vec<usize>>,
            _filters: &[Expr],
            _limit: Option<usize>,
        ) -> DfResult<Arc<dyn DfExecutionPlan>> {
            let schema = Schema::try_from(self.schema()).unwrap();
            let my_plan = Arc::new(MyExecutionPlan {
                schema: Arc::new(schema),
            });
            let df_plan = DfExecutionPlanAdapter(my_plan);
            Ok(Arc::new(df_plan))
        }
    }

    #[derive(Debug)]
    struct MyExecutionPlan {
        schema: SchemaRef,
    }

    #[async_trait]
    impl ExecutionPlan for MyExecutionPlan {
        fn as_any(&self) -> &dyn Any {
            self
        }

        fn schema(&self) -> SchemaRef {
            self.schema.clone()
        }

        fn output_partitioning(&self) -> Partitioning {
            Partitioning::UnknownPartitioning(1)
        }

        fn output_ordering(&self) -> Option<&[PhysicalSortExpr]> {
            None
        }

        fn children(&self) -> Vec<ExecutionPlanRef> {
            vec![]
        }

        fn with_new_children(&self, _children: Vec<ExecutionPlanRef>) -> Result<ExecutionPlanRef> {
            unimplemented!()
        }

        async fn execute(
            &self,
            _partition: usize,
            _runtime: Arc<RuntimeEnv>,
        ) -> Result<SendableRecordBatchStream> {
            let schema = self.schema();
            let recordbatches = RecordBatches::try_new(
                schema.clone(),
                vec![
                    RecordBatch::new(
                        schema.clone(),
                        vec![Arc::new(Int32Vector::from_slice(vec![1])) as _],
                    )
                    .unwrap(),
                    RecordBatch::new(
                        schema,
                        vec![Arc::new(Int32Vector::from_slice(vec![2, 3])) as _],
                    )
                    .unwrap(),
                ],
            )
            .unwrap();
            Ok(recordbatches.as_stream())
        }

        fn statistics(&self) -> Statistics {
            Statistics::default()
        }
    }

    // Test our execution plan can be executed by DataFusion, through adapters.
    #[tokio::test]
    async fn test_execution_plan_adapter() {
        let ctx = ExecutionContext::new();
        let logical_plan = LogicalPlanBuilder::scan("test", Arc::new(MyDfTableProvider), None)
            .unwrap()
            .build()
            .unwrap();
        let physical_plan = ctx.create_physical_plan(&logical_plan).await.unwrap();
        let df_recordbatches = collect(physical_plan, Arc::new(RuntimeEnv::default()))
            .await
            .unwrap();
        let pretty_print = arrow_print::write(&df_recordbatches);
        let pretty_print = pretty_print.lines().collect::<Vec<&str>>();
        assert_eq!(
            pretty_print,
            vec!["+---+", "| a |", "+---+", "| 1 |", "| 2 |", "| 3 |", "+---+",]
        );
    }
}
